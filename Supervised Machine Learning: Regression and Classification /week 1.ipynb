{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Supervised vs. Unsupervised Learning\n",
    "===============================\n",
    "- Supervised Learning: training data includes the correct answer\n",
    "    - Classification: identify which category an object belongs to\n",
    "    - Regression: predict a continuous value\n",
    "        - Linear Regression\n",
    "        - Logistic Regression\n",
    "        \n",
    "- Unsupervised Learning: training data is not labeled\n",
    "    - Clustering: group similar objects"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7c6ce2987d95514"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear Regression with One Variable\n",
    "===============================\n",
    "f(x) = wx + b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d719e88ada46a0a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "w = 4\n",
    "b = 3\n",
    "\n",
    "# Plot f(x) along the data\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, w*X + b, color='red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cost Function\n",
    "===============================\n",
    "m = number of training examples\n",
    "J(w, b) = 1/2m * sum(y^ â€” y)^2\n",
    "J(w, b) = 1/2m * sum((wx + b - y)^2)\n",
    "\n",
    "Goal: minimize J(w, b)\n",
    "in the case of linear regression, we can use the Mean Squared Error (MSE) as the cost function\n",
    "\n",
    "in the following example, we can see that at w=4 and b=3, the cost function is at its minimum\n",
    "that is, the red cross is at the lowest point of the cost function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15b6876984b2ff58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    J(w, b) = 1/2m * sum((wx + b - y)^2)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    J = 1/(2*m) * np.sum((w*X + b - y)**2)\n",
    "    return J\n",
    "\n",
    "# iterate over w and b to plot the cost function\n",
    "w_vals = np.linspace(0, 8, 100)\n",
    "b_vals = np.linspace(0, 8, 100)\n",
    "J_vals = np.zeros((len(w_vals), len(b_vals)))\n",
    "\n",
    "for i in range(len(w_vals)):\n",
    "    for j in range(len(b_vals)):\n",
    "        J_vals[i, j] = compute_cost(X, y, w_vals[i], b_vals[j])\n",
    "\n",
    "plt.contourf(w_vals, b_vals, J_vals, levels=20, cmap='viridis')\n",
    "plt.plot(4, 3, c='red', marker='x')\n",
    "plt.colorbar()\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bc2f3163158fa59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient Descent\n",
    "===============================\n",
    "- Start with some initial values for w and b\n",
    "- Keep changing w and b to reduce J(w, b) until we hopefully end up at a minimum\n",
    "- The direction to move w and b is given by the partial derivative of J(w, b) with respect to w and b\n",
    "- The size of the step is determined by the learning rate\n",
    "- The learning rate should be chosen carefully\n",
    "- If the learning rate is too small, the algorithm will take a long time to converge\n",
    "- If the learning rate is too large, the algorithm may overshoot the minimum\n",
    "\n",
    "Batch Gradient Descent: compute the gradient of the cost function with respect to all training examples\n",
    "\n",
    "algorithm:\n",
    "- initialize w and b\n",
    "- repeat until convergence:\n",
    "    - w = w - alpha * dJ(w, b)/dw\n",
    "    - b = b - alpha * dJ(w, b)/db\n",
    "\n",
    "where alpha is the learning rate, and w and b are updated simultaneously"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a5d5a9812d18b71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    dJ(w, b)/dw = 1/m * sum((wx + b - y) * x)\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    dw = 1/m * np.sum((w*X + b - y) * X)\n",
    "    db = 1/m * np.sum(w*X + b - y)\n",
    "    return dw, db\n",
    "\n",
    "convergence_threshold = 0.00001\n",
    "\n",
    "def gradient_descent(X, y, w, b, alpha):\n",
    "    iter_counter = 0\n",
    "    J = compute_cost(X, y, w, b)\n",
    "    while True:\n",
    "        iter_counter += 1\n",
    "        dw, db = compute_gradient(X, y, w, b)\n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "        J_new = compute_cost(X, y, w, b)\n",
    "        if abs(J - J_new) < convergence_threshold:\n",
    "            print(f'Converged after {iter_counter} iterations')\n",
    "            break\n",
    "        J = J_new\n",
    "    return w, b\n",
    "    \n",
    "alpha = 0.3\n",
    "w, b = gradient_descent(X, y, 0, 0, alpha)\n",
    "print(w, b)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "562522015acca24d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
