{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Reinforcement Learning\n",
    "======================\n",
    "RL is a type of machine learning that allows an agent to learn how to behave in an environment by performing actions and receiving rewards. The agent learns to achieve a goal in an uncertain, potentially complex environment. In RL, an agent learns from trial and error to achieve a clear objective. The agent learns from its experiences and interactions with the environment. The agent learns from the consequences of its actions, rather than from being told what to do.\n",
    "We have states, actions, and rewards. The agent interacts with the environment by taking actions. The agent receives rewards by performing actions. The agent learns to achieve a goal by maximizing rewards. The agent learns a policy that maps states to actions. The agent learns a value function that maps states or state-action pairs to rewards. The agent learns a model of the environment that predicts the next state and reward given the current state and action.\n",
    "Some applications include:\n",
    "- controlling robots\n",
    "- factory optimization\n",
    "- financial (stock) trading\n",
    "- playing games (including video games)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a0b3eff5a2ab3d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mars Rover Example\n",
    "==================\n",
    "Let's consider the example of a Mars rover. The rover is an agent that learns to navigate the Martian surface. The rover has states (locations on the surface), actions (movements in different directions), and rewards (scientific discoveries). The rover learns to explore the surface to maximize its scientific discoveries. The rover learns a policy that tells it where to go next. The rover learns a value function that tells it how valuable each location is. The rover learns a model of the environment that predicts what it will find at each location.\n",
    "We will show the transitions with a 4-parameter tuple: (state, action, reward, next_state)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55cac71016869540"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Return in RL\n",
    "-----------------\n",
    "The return in RL is the sum of the rewards that the agent receives over time, but with discount factor, a number a little less than 1.\n",
    "The formula for the return is:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "where:\n",
    "- $G_t$ is the return at time $t$\n",
    "- $R_t$ is the reward at time $t$\n",
    "- $\\gamma$ is the discount factor\n",
    "What the gamma does is to make the rewards that are further in the future less important.\n",
    "So the return helps us to measure how good a state or an action is, helping the machine to decide what to do at each step."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b044386af22a599f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Policy\n",
    "-------\n",
    "The policy in RL is a function that maps states to actions. The policy tells the agent what action to take in each state. The policy can be deterministic (always choose the same action in a given state) or stochastic (choose different actions with different probabilities in a given state).\n",
    "So the policy pi(s)=a is a function mapping from states to actions, that tells you what action to take in each state.\n",
    "The goal in RL is to find a policy that maximizes the expected return."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "164349ef375ae4f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: All the concepts introduced above are called a 'Markov Decision Process' (MDP).\n",
    "Markov means that the future is independent of the past given the present."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30d4d2d57979ba7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "State-Action Value Function\n",
    "----------------------------\n",
    "The state-action value function (also called Q-function) is a function that maps state-action pairs to values. The Q-function tells us how good it is to take a particular action in a particular state. The Q-function is defined as the expected return of taking an action in a state and then following a policy.\n",
    "Q(s, a) = Return, if you:\n",
    "- start in state s\n",
    "- take action a\n",
    "- then behave optimally after that\n",
    "-> The best possible return from state s is max(Q(s, a)) on a.\n",
    "-> The best possible action in state s is the action a that gives max(Q(s, a)).\n",
    "\n",
    "So if we can compute the Q-function, we can find the best policy by choosing the action that maximizes Q in each state."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "156ee7c5493beaff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bellman Equation\n",
    "----------------\n",
    "The Bellman equation is a fundamental equation in RL that decomposes the Q-function into two parts: the immediate reward and the discounted future reward (return from behaving optimally starting from state s').\n",
    "The Bellman equation for the Q-function is:\n",
    "$$Q(s, a) = R + \\gamma \\max_{a'} Q(s', a')$$\n",
    "where:\n",
    "- Q(s, a) is the Q-function for state s and action a\n",
    "- R is the immediate reward for taking action a in state s\n",
    "- $\\gamma$ is the discount factor\n",
    "- s' is the next state after taking action a in state s\n",
    "- a' is the next action after taking action a in state s"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a38778d252d4a33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random (Stochastic) Environment\n",
    "-------------------------------\n",
    "In reality, most of the time the environment is stochastic, meaning that the outcome of an action is not deterministic. In a stochastic environment, the agent cannot predict with certainty what will happen when it takes an action. There is some randomness in the environment. For example, a robot might slip on a wet floor, or a self-driving car might encounter unexpected traffic.\n",
    "\n",
    "Expected Return = Average(R1 + gamma*R2 + gamma^2*R3 + ...)\n",
    "                = E[R1 + gamma*R2 + gamma^2*R3 + ...]\n",
    "                \n",
    "So the goal of RL is to find a policy that maximizes the expected return.\n",
    "Bellman Equation becomes:\n",
    "$$Q(s, a) = R + E[\\gamma \\max_{a'} Q(s', a')]$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "840539d5fdf8b6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: In many applications the state space is not discrete, but continuous."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b619ca27e1f4ef4"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2f353587ed2f5796"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
