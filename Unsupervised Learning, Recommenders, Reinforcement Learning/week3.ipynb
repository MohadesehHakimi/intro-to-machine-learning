{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Reinforcement Learning\n",
    "======================\n",
    "RL is a type of machine learning that allows an agent to learn how to behave in an environment by performing actions and receiving rewards. The agent learns to achieve a goal in an uncertain, potentially complex environment. In RL, an agent learns from trial and error to achieve a clear objective. The agent learns from its experiences and interactions with the environment. The agent learns from the consequences of its actions, rather than from being told what to do.\n",
    "We have states, actions, and rewards. The agent interacts with the environment by taking actions. The agent receives rewards by performing actions. The agent learns to achieve a goal by maximizing rewards. The agent learns a policy that maps states to actions. The agent learns a value function that maps states or state-action pairs to rewards. The agent learns a model of the environment that predicts the next state and reward given the current state and action.\n",
    "Some applications include:\n",
    "- controlling robots\n",
    "- factory optimization\n",
    "- financial (stock) trading\n",
    "- playing games (including video games)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a0b3eff5a2ab3d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mars Rover Example\n",
    "==================\n",
    "Let's consider the example of a Mars rover. The rover is an agent that learns to navigate the Martian surface. The rover has states (locations on the surface), actions (movements in different directions), and rewards (scientific discoveries). The rover learns to explore the surface to maximize its scientific discoveries. The rover learns a policy that tells it where to go next. The rover learns a value function that tells it how valuable each location is. The rover learns a model of the environment that predicts what it will find at each location.\n",
    "We will show the transitions with a 4-parameter tuple: (state, action, reward, next_state)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55cac71016869540"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Return in RL\n",
    "-----------------\n",
    "The return in RL is the sum of the rewards that the agent receives over time, but with discount factor, a number a little less than 1.\n",
    "The formula for the return is:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "where:\n",
    "- $G_t$ is the return at time $t$\n",
    "- $R_t$ is the reward at time $t$\n",
    "- $\\gamma$ is the discount factor\n",
    "What the gamma does is to make the rewards that are further in the future less important.\n",
    "So the return helps us to measure how good a state or an action is, helping the machine to decide what to do at each step."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b044386af22a599f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Policy\n",
    "-------\n",
    "The policy in RL is a function that maps states to actions. The policy tells the agent what action to take in each state. The policy can be deterministic (always choose the same action in a given state) or stochastic (choose different actions with different probabilities in a given state).\n",
    "So the policy pi(s)=a is a function mapping from states to actions, that tells you what action to take in each state.\n",
    "The goal in RL is to find a policy that maximizes the expected return."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "164349ef375ae4f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: All the concepts introduced above are called a 'Markov Decision Process' (MDP).\n",
    "Markov means that the future is independent of the past given the present."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30d4d2d57979ba7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "State-Action Value Function\n",
    "----------------------------\n",
    "The state-action value function (also called Q-function) is a function that maps state-action pairs to values. The Q-function tells us how good it is to take a particular action in a particular state. The Q-function is defined as the expected return of taking an action in a state and then following a policy.\n",
    "Q(s, a) = Return, if you:\n",
    "- start in state s\n",
    "- take action a\n",
    "- then behave optimally after that\n",
    "-> The best possible return from state s is max(Q(s, a)) on a.\n",
    "-> The best possible action in state s is the action a that gives max(Q(s, a)).\n",
    "\n",
    "So if we can compute the Q-function, we can find the best policy by choosing the action that maximizes Q in each state."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "156ee7c5493beaff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bellman Equation\n",
    "----------------\n",
    "The Bellman equation is a fundamental equation in RL that decomposes the Q-function into two parts: the immediate reward and the discounted future reward (return from behaving optimally starting from state s').\n",
    "The Bellman equation for the Q-function is:\n",
    "$$Q(s, a) = R + \\gamma \\max_{a'} Q(s', a')$$\n",
    "where:\n",
    "- Q(s, a) is the Q-function for state s and action a\n",
    "- R is the immediate reward for taking action a in state s\n",
    "- $\\gamma$ is the discount factor\n",
    "- s' is the next state after taking action a in state s\n",
    "- a' is the next action after taking action a in state s"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a38778d252d4a33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random (Stochastic) Environment\n",
    "-------------------------------\n",
    "In reality, most of the time the environment is stochastic, meaning that the outcome of an action is not deterministic. In a stochastic environment, the agent cannot predict with certainty what will happen when it takes an action. There is some randomness in the environment. For example, a robot might slip on a wet floor, or a self-driving car might encounter unexpected traffic.\n",
    "\n",
    "Expected Return = Average(R1 + gamma*R2 + gamma^2*R3 + ...)\n",
    "                = E[R1 + gamma*R2 + gamma^2*R3 + ...]\n",
    "                \n",
    "So the goal of RL is to find a policy that maximizes the expected return.\n",
    "Bellman Equation becomes:\n",
    "$$Q(s, a) = R + E[\\gamma \\max_{a'} Q(s', a')]$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "840539d5fdf8b6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: In many applications the state space is not discrete, but continuous."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b619ca27e1f4ef4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Learning the state-value function\n",
    "=================================\n",
    "The idea is to use a neural network to approximate the Q-function. The neural network takes a state s and actions a as input and outputs Q(s, a). --> Deep Reinforcement Learning\n",
    "But how do we train this network?\n",
    "\n",
    "We can use the Bellman equation: Q(s, a) = R(s) + gamma * max(Q(s', a')).\n",
    "Remember that for a supervised learning network the data must be in the form: f_wb(x) = y\n",
    "To make the dataset, we try various states and actions, and for each state and action we calculate the target value using the Bellman equation.\n",
    "So we will have:\n",
    "(s(i), a(i), R(s(i)), s'(i))\n",
    "We can then make a dataset, with the first two values as x, and the last two as y. --> x(i) = s(i), a(i), y(i) = R(s(i)) + gamma * max(Q(s', a'))\n",
    "\n",
    "But what is the Q-function? We don't know! It will be just some random guess and then over time, it will learn the correct values.\n",
    "\n",
    "Learning Algorithm\n",
    "------------------\n",
    "- Initialize the neural network with random weights as guess of Q(s, a)\n",
    "- Repeat {\n",
    "-     *Take actions in the environment to get (s, a, R, s')\n",
    "-     Store the 10,000 most recent experiences in a replay buffer\n",
    "-     Train the network:\n",
    "-         Create training set of 10,000 examples using x = (s, a) and y = R(s) + gamma * max(Q(s', a'))\n",
    "-         Train Q_new (the new neural network) such that Q_new(s, a) ~ y\n",
    "-     Set Q = Q_new.\n",
    "- }\n",
    "The algorithm is called Deep Q-Network (DQN)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2854a028a60e6417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algorithm Refinement\n",
    "--------------------\n",
    "In the previous network architecture, we had to run the network 4 times to get Q(s, a) for different possible actions.\n",
    "Turns out we can instead have a network that outputs these 4 values at once.\n",
    "In this new architecture, the network takes in a state and outputs Q(s, a) for all possible actions. We will pick the action with the highest Q-value."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdef77d36193da50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algorithm Refinement: epsilon-greedy policy\n",
    "-------------------------------------------\n",
    "How do we choose the action to take even while we're still learning?\n",
    "Notice that in the above described algorithm, there is a starred step where we take actions in the environment to get (s, a, R, s').\n",
    "Taking actions totally randomly is not a good option. We can pick the action a that maximizes Q, but still isn't the best option.\n",
    "Instead, we can use probability to decide which action to take.\n",
    "This way we can explore the environment more efficiently.\n",
    "So the algorithm changes such that most of the time, say, with 0.95 probability, pick the action a that maximizes Q (Greedy/Exploitation), and with 0.05 probability pick a random action (Exploration).\n",
    "This way, even if the network got stuck in some local minimum, it can still explore the environment. (For example, it might think that some action is bad due to the initial random guess, but it might actually be good.)\n",
    "\n",
    "In this algorithm, epsilon is the probability of taking a random action. (Here, 5%)\n",
    "One common exercise is to start with a high epsilon value and then decrease it over time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19eaa622fc6da6db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Tuning the hyperparameters of RL algorithms are usually trickier than supervised learning algorithms; probably because of the less mature nature of the field."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7f8ad23e58789e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Algorithm Refinement: Mini-Batch and Soft Updates\n",
    "-------------------------------------------------\n",
    "Mini-Batch gradient descent: Instead of training the algorithm on the entire dataset, we can train it on a subset of the data. This is called Mini-Batch gradient descent. This way, the network can learn faster, specially when the dataset is large.\n",
    "So in the DQN algorithm, we can use Mini-Batch gradient descent to train the network -> use 1000 instead of 10000 examples.\n",
    "\n",
    "Soft Update: Instead of updating the network with the new weights, we can update it with a fraction of the new weights. This way the network can learn more slowly, but more steadily. This way we can avoid updating Q to a worse value.\n",
    "W = tau * W + (1 - tau) * W'\n",
    "where:\n",
    "- W is the current weights\n",
    "- W' is the new weights\n",
    "- tau is the fraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "200ee4eab142175a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
