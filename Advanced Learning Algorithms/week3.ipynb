{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Machine Learning Diagnostic\n",
    "===========================\n",
    "A test to run to gain insight what is/isn't working with a learning algorithm, and gain guidance as to how best to improve its performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7abd608c277d9dd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating a Model\n",
    "===================================\n",
    "- A model can be evaluated using a training set and a test set.\n",
    "- The training set is used to train the model.\n",
    "- The test set is used to evaluate the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31e22f6b9aca74b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train/test procedure for linear regression\n",
    "==========================================\n",
    "1. Learn parameter theta from training data (minimize J(theta)).\n",
    "2. Compute test set error:\n",
    "3. For linear regression: J(test) = 1/2m_test * sum((h_theta(x_test(i)) - y_test(i))^2)\n",
    "4. in which h_theta(x) = theta' * x, theta' is the learned parameter, x is the input, y is the output.\n",
    "5. Compute training set error:\n",
    "6. For linear regression: J(train) = 1/2m_train * sum((h_theta(x_train(i)) - y_train(i))^2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1da81f80592b1c37"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train/test procedure for logistic regression\n",
    "============================================\n",
    "1. Learn parameter theta from training data (minimize J(theta)).\n",
    "2. Compute test set error:\n",
    "3. For logistic regression: J(test) = -1/m_test * sum(y_test(i) * log(h_theta(x_test(i))) + (1 - y_test(i)) * log(1 - h_theta(x_test(i))))\n",
    "4. in which h_theta(x) = g(theta' * x), g(z) = 1 / (1 + e^(-z)), theta' is the learned parameter, x is the input, y is the output.\n",
    "5. Compute training set error:\n",
    "6. For logistic regression: J(train) = -1/m_train * sum(y_train(i) * log(h_theta(x_train(i))) + (1 - y_train(i)) * log(1 - h_theta(x_train(i))))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710b57384c668616"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For classification problems, there are other definitions of J(test) and J(train) that are maybe more common.\n",
    "They instead measure the fraction of the test set and the training set that are misclassified.\n",
    "J(test): fraction of the test set that are misclassified.\n",
    "J(train): fraction of the training set that are misclassified."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0929f0da0beeb28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In an over-fitted model, the training error will be low, but the test error will be high.\n",
    "So this is a way to evaluate the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "928362a0de4b71ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Selection\n",
    "================\n",
    "- How to choose the model?\n",
    "- For example, how to choose the degree of the polynomial?\n",
    "- One way is to split the data into three parts: training set, cross-validation set, and test set.\n",
    "- Use the training set to learn the parameters.\n",
    "- Then choose the degree of the polynomial using the cross-validation set: try different degrees of the polynomial, and choose the one that gives the lowest cross-validation error.\n",
    "- Finally, estimate the generalization error using the test set: this is the error of the model on new data.\n",
    "\n",
    "- This procedure can be used for other models as well, such as neural networks.\n",
    "- Cross validation set is also called validation, development, or dev set.\n",
    "Note: When selecting a model, you want to choose one that performs well both on the training and cross-validation set. It implies that it is able to learn the patterns from your training set without over-fitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "642bdd78fd3d93a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A note on feature scaling:\n",
    "\n",
    "As with the training set, you will also want to scale the cross-validation and test sets. An important thing to note when using the z-score is you have to use the mean and standard deviation of the training set when scaling the cross-validation and test sets. This is to ensure that your input features are transformed as expected by the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "578aa2985aec1d24"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "49d4963e90d341da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bias and Variance\n",
    "=================\n",
    "- Bias and variance are two sources of error in a model.\n",
    "- High bias is under-fitting: the model is too simple to capture the underlying structure of the data.\n",
    "- High variance is over-fitting: the model is too complex and captures noise in the data.\n",
    "- High bias(underfit): training error is high, cross-validation error is high ----> J(train) ~ J(cross-validation)\n",
    "- High variance(overfit): training error may be low, cross-validation error is much higher ----> J(train) << J(cross-validation)\n",
    "- High bias and high variance: training error is high, cross-validation error is much higher ----> J(train) << J(cross-validation)\n",
    "- The optimal model will have low training error and low cross-validation error.\n",
    "\n",
    "- The training error decreases as the model complexity increases.\n",
    "- The cross-validation error decreases as the model complexity increases, but then increases as the model becomes too complex."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "826959ef62e82d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Regularization and Bias/Variance\n",
    "=================================\n",
    "How to choose the regularization parameter lambda?\n",
    "The procedure is similar to choosing the degree of the polynomial.\n",
    "- Split the data into three parts: training set, cross-validation set, and test set.\n",
    "- Use the training set to learn the parameters.\n",
    "- Choose the regularization parameter lambda using the cross-validation set.\n",
    "- Estimate the generalization error using the test set.\n",
    "So, try different values of lambda and choose the one that gives the lowest cross-validation error."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e615a38aa653ac91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we were to draw a graph of the training error and the cross-validation error as a function of the regularization parameter lambda, we would see that:\n",
    "- The training error increases as lambda increases.\n",
    "- The cross-validation error decreases as lambda increases.\n",
    "- The cross-validation error is high when lambda is too low or too high.\n",
    "The graph would be a mirror image of the graph of the training error and the cross-validation error as a function of the model complexity."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572bb694b439debb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Establishing a Baseline Level of Performance\n",
    "==============================================\n",
    "- We need concrete numbers to determine if a learning algorithm has high bias or high variance.\n",
    "- To judge if the training error is high, we establish a baseline level of performance.\n",
    "- If the training error is much higher than the baseline level, it indicates a high bias problem.\n",
    "- If the training error is low, but the cross-validation error is much higher than the training error, it indicates a high variance problem.\n",
    "- If both the training error and the cross-validation error are high, it indicates a high bias and high variance problem."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2906613bdc3f7845"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some common ways to set a baseline level of performance:\n",
    "- Human-level performance: If humans can do the task, then the error they make is a good baseline.\n",
    "- Competing models: If there are other models that can do the task, then the error they make is a good baseline.\n",
    "- Guess based on experience"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "213e3cd8f3476378"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Learning Curves\n",
    "================\n",
    "- Learning curves are a good way to diagnose bias and variance problems.\n",
    "- They plot the training error and the cross-validation error as a function of the training set size.\n",
    "- Usually, the training error increases as the training set size increases, because it is harder to fit a larger training set.\n",
    "- The cross-validation error decreases as the training set size increases.\n",
    "- The cross-validation error is higher than the training error, but they tend to converge as the training set size increases.\n",
    "- High bias: The J(train) increases rapidly, but tends to flatten out. The J(cv) decreases rapidly, but tends to flatten out. J(cv) is still higher. If you set a baseline level of performance, you can see a large gap between that and J(train); we know that this indicates high bias.\n",
    "- The flattening out of the curves indicates that the model is just too simple, and it doesn't really matter how much data you feed it. So for a high bias problem, more training data is not the solution. You need to try a more complex model.\n",
    "- High variance: The J(train) is low, but the J(cv) is high. The two curves are far apart. If you set a baseline level of performance, you can see a large gap between that and J(cv); we know that this indicates high variance.\n",
    "- In a high variance problem, the model is too complex, and it is over-fitting the training data. It turns out that if you feed it more training data, the model will generalize better. So for a high variance problem, more training data is likely the solution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c176e65cde92d912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "High bias:\n",
    "- Try getting additional features\n",
    "- Try adding polynomial features\n",
    "- Try decreasing lambda\n",
    "\n",
    "High variance:\n",
    "- Get more training examples\n",
    "- Try smaller sets of features\n",
    "- Try increasing lambda"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9c6ea748bc18866"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bias/Variance and Neural Networks\n",
    "=================================\n",
    "- We have seen the bias variance tradeoff.\n",
    "- Turns out that neural networks offer us a way out of this tradeoff.\n",
    "- Large neural networks with small to moderate sized datasets are low bias machines; meaning that if your neural network is large enough, it can fit the training set well, so long as the dataset is not enormous.\n",
    "- The method we can use is as follows:\n",
    "1. Does the model do well on the training set? -> high bias\n",
    "2. If not, increase the size of the neural network.\n",
    "3. Repeat until the model does well on the training set.\n",
    "4. Does the model do well on the cross-validation set? -> high variance\n",
    "5. If not, increase the size of the training set.\n",
    "6. Go to step 1.\n",
    "7. If the model does well on the training and cross-validation sets, then you have a good model!\n",
    "- Remember that for steps 1 and 4, we use the baseline performance we talked about earlier."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a08123224293ee6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Does increasing the size of the neural network cause over-fitting?\n",
    "It turns out that a large neural network will usually do as well or better than a smaller one so long as regularization is chosen appropriately."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b05ecfc300bdd5c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neural Network Regularization\n",
    "==============================\n",
    "J(w, b) = 1/m * sum(L(f(x(i)), y(i))) + {lambda/2m * sum(w^2)}: regularization term\n",
    "- L(f(x(i)), y(i)) is the loss function.\n",
    "- Regularized model in Tenserflow:\n",
    "- Dense(units=..., activation=..., kernel_regularizer=L2(lambda))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4506448366b2b9d1"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4c76c455f51c60b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Machine Learning Development Process\n",
    "====================================\n",
    "The iterative loop of ML development:\n",
    "- Choose architecture: model, data, hyperparameters, etc.\n",
    "- Train the model.\n",
    "- Diagnostics: bias, variance and error analysis.\n",
    "- Go through the loop until you have a good model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6966d47109662862"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Error Analysis\n",
    "================\n",
    "- Error analysis refers to the process of manually examining the errors that the model makes.\n",
    "- This can give you insight into what is/isn't working with the model.\n",
    "- For example, in the case of spam email classification, say the model is misclassifying 100 emails.\n",
    "- You could manually examine these emails to see if there is a pattern.\n",
    "- Categorize them based on common traits:\n",
    "- E.g. pharma related emails are about 50% of the errors, but deliberate misspelling is only 5%.\n",
    "- This can help you decide what to do next.\n",
    "- You might decide to get more data on pharma related emails, or create more features, \n",
    "\n",
    "- The categories might be overlapping, so one email can fall into multiple categories.\n",
    "- If the amount of misclassified emails is large, you can sample a subset of them.\n",
    "- One downside of error analysis is that it's much easier to do for problems that humans are good at."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df77cf2aae05fe32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding Data\n",
    "============\n",
    "- Add more data of the types where error analysis has indicated it might help.\n",
    "- Besides getting new data, another technique is data augmentation.\n",
    "- Augmentation: modifying an existing data set to create new data.\n",
    "- For example for OCR: rotate, scale, or skew the images. Or you can place a grid on top of the images and randomly warp the images.\n",
    "- for speech recognition: add background noise.\n",
    "\n",
    "- Distortions should be reasonable, so that the data is still representative of the real data or the test set.\n",
    "- Usually does not help to add purely random/meaningless noise to the data.\n",
    "\n",
    "\n",
    "- Data synthesis: using artificial data inputs to create a new data set.\n",
    "- For example for OCR: using fonts on your computer, create new data.\n",
    "- Data synthesis is usually used for vision problems.\n",
    "\n",
    "- Conventional model-centric approach: focus on improving the model.\n",
    "- Data-centric approach: focus on improving the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd794b860bf56d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transfer Learning\n",
    "==================\n",
    "Sometimes you don't have that much data and getting data is expensive or challenging.\n",
    "- Transfer learning is a way to use a model trained on one task and apply it to a different task.\n",
    "- For example, you have a model trained to classify 1000 categories of objects in images.\n",
    "- You can use this model to classify digits from 0 to 9.\n",
    "- You take the pre-trained model and remove the last layer.\n",
    "- You add a new output layer that will classify the digits.\n",
    "- There are two options:\n",
    "- Option 1: Only train the new output layer -> good for a very small dataset.\n",
    "- Option 2: Retrain the entire model -> good for a larger dataset.\n",
    "\n",
    "- So in transfer learning we have two tasks:\n",
    "- Supervised pre-training and then Fine-tuning.\n",
    "\n",
    "- But why does transfer learning work?\n",
    "- The intuition is that the features learned in the lower layers are more general, e.g., the edges, curves, etc.\n",
    "- One restriction of pre-trained models is that the input type should be the same: image, text, audio, etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d348be79df3cba6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Full Cycle of a Machine Learning Project\n",
    "========================================\n",
    "1. Scope project: define the goal\n",
    "2. Data collection: define and collect data\n",
    "3. Train the model: training, error analysis and iterative improvement\n",
    "4. Go to step 2 until you have a good model\n",
    "5. Deploy the model in production: deploy, monitor, and maintain the system\n",
    "6. Go to step 2 or 3 if needed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "677ddf040183cd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Skewed Datasets\n",
    "================\n",
    "- Sometimes the ratio of positive to negative examples is very high and far from 50/50.\n",
    "- In these cases, metrics like accuracy are not very informative.\n",
    "- Consider the case of a classifier for a rare disease that is present for only 0.5% of the population.\n",
    "- If the classifier predicts that no one has the disease, it will have an accuracy of 99.5%.\n",
    "\n",
    "- A good pair of metrics to use in this case is precision/recall.\n",
    "- For that, we draw a table with four cells: true positive, false positive, true negative, false negative.\n",
    "- Precision: of all the patients with the rare disease, what fraction actually have the disease?\n",
    "- Precision = true positive / (true positive + false positive)\n",
    "- Recall: of all the patients that actually have the disease, what fraction did the model correctly identify?\n",
    "- Recall = true positive / (true positive + false negative)\n",
    "- These metrics are desired to be high."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "474b30aa8a727999"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trading off Precision and Recall\n",
    "=================================\n",
    "- These metrics are both desired to be high.\n",
    "- In practice, there is a trade-off between precision and recall.\n",
    "- Suppose we're using logistic regression to classify the presence of the rare disease.\n",
    "- We would like to predict y=1 only if we are very confident.\n",
    "- That means setting the threshold higher, like 0.7 instead of the usual 0.5.\n",
    "- This will increase precision, but decrease recall.\n",
    "- On the other hand, if we want to catch all the cases of the disease, we can set the threshold lower.\n",
    "- This will increase recall, but decrease precision."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "326ce385ff2fc89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotting precision and recall against different values of the threshold gives a precision-recall curve.\n",
    "- The precision-recall curve is a good way to compare different models and pick the best threshold.\n",
    "- It turns out that if you want to automatically trade off precision and recall, you can use the F1 score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51a1fa7bbb1909c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "F1 Score\n",
    "=========\n",
    "- The F1 score is the harmonic mean of precision and recall.\n",
    "- F1 = 2 * precision * recall / (precision + recall)\n",
    "- Or,\n",
    "- F1 = 2 / (1 / precision + 1 / recall)\n",
    "- The F1 score pays more attention to the smaller of the two values, because if one of them is very low, the F1 score will be very low and the model is probably not good."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc825b0a326c507a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
